<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>HIS</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website"/>
    <meta property="og:title" content="HIS"/>
    <meta property="og:description" content="HIS"/>
    <link rel="icon" type="image/png" href="../../images/logo_uva.png">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="js/app.js"></script>
</head>

<body>
<div class="container" id="main">
    <div class="row">
        <h2 class="col-md-12 text-center">
            <b>Hyperbolic Image Segmentation</b><br>
            <small>
                CVPR 2022
            </small>
        </h2>
    </div>
    <div class="row">
        <div class="col-md-12 text-center">
            <ul class="list-inline">
                <li>
                    <a href="https://minaghadimi.github.io/">
                        <b>Mina Ghadimi Atigh</b>
                    </a>
                    <br><b>UvA</b>
                </li>
                <li>
                    <a href="">
                        <b>Julian Schoep</b>
                    </a>
                    <br><b>UvA</b>
                </li>
                <br><br>
                <li>
                    <a href="https://sites.google.com/view/ermanacar/">
                        <b>Erman Acar</b>
                    </a>
                    <br><b>Leiden University, </b>
                    <b>Vrije Universiteit Amsterdam</b>
                </li>
                <br><br>
                <li>
                    <a href="https://nanne.github.io/">
                        <b>Nanne van Noord</b>
                    </a>
                    <br><b>UvA</b>
                </li>
                <li>
                    <a href="https://staff.fnwi.uva.nl/p.s.m.mettes/">
                        <b>Pascal Mettes</b>
                    </a>
                    </br><b>UvA</b>
                </li>
                <br><br>
            </ul>
        </div>
    </div>


    <div class="row">
        <div class="col-md-4 col-md-offset-4 text-center">
            <ul class="nav nav-pills nav-justified">
                <li>
                    <a href="https://arxiv.org/pdf/2203.05898.pdf">
                        <img src="img/HIS.jpg" height="60px">
                        <h4><strong>Paper</strong></h4>
                    </a>
                </li>
<!--                <li>-->
<!--                    <a href="">-->
<!--                        <img src="img/youtube_icon.png" height="60px">-->
<!--                        <h4><strong>Video</strong></h4>-->
<!--                    </a>-->
<!--                </li>-->
                <li>
                    <a href="https://github.com/MinaGhadimiAtigh/HyperbolicImageSegmentation">
                        <img src="img/github.png" height="60px">
                        <h4><strong>Code</strong></h4>
                    </a>
                </li>
                <li>
                    <a href="img/08683-supp.pdf">
                        <img src="img/HIS_sup.jpg" height="60px">
                        <h4><strong>Supplementary Material</strong></h4>
                    </a>
                </li>
            </ul>
        </div>
    </div>


    <div class="row">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                Abstract
            </h3>
            <!--            <img src="img/rays.png" class="img-responsive" alt="overview"><br>-->
            <p class="text-justify">
                For image segmentation, the current standard is to perform pixel-level optimization and inference
                in Euclidean output embedding spaces through linear hyperplanes. In this work, we show that hyperbolic
                manifolds provide a valuable alternative for image segmentation and propose a tractable formulation of
                hierarchical pixel-level classification in hyperbolic space. Hyperbolic Image Segmentation opens up new
                possibilities and practical benefits for segmentation, such as uncertainty estimation and boundary
                information for free, zero-label generalization, and increased performance in low-dimensional output
                embeddings.
            </p>
            <img src="../../images/HIS.jpeg" class="img-responsive" alt="overview"><br>
        </div>

    </div>


    <!--    <div class="row">-->
    <!--        <div class="col-md-8 col-md-offset-2">-->
    <!--            <h3>-->
    <!--                Video-->
    <!--            </h3>-->
    <!--            <div class="text-center">-->
    <!--                <div style="position:relative;padding-top:56.25%;">-->
    <!--                    <iframe src="https://www.youtube.com/embed/EpH175PY1A0" allowfullscreen-->
    <!--                            style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>-->
    <!--                </div>-->
    <!--            </div>-->
    <!--        </div>-->
    <!--    </div>-->


    <!--    <div class="row">-->
    <!--        <div class="col-md-8 col-md-offset-2">-->
    <!--            <h3>-->
    <!--                Integrated Positional Encoding-->
    <!--            </h3>-->
    <!--            <p class="text-justify">-->
    <!--                Typical positional encoding (as used in Transformer networks and Neural Radiance Fields) maps a single-->
    <!--                point in space to a feature vector, where each element is generated by a sinusoid with an exponentially-->
    <!--                increasing frequency:-->
    <!--            </p>-->
    <!--            <p style="text-align:center;">-->
    <!--                <img src="img/pe_seq_eqn_pad.png" height="50px" class="img-responsive">-->
    <!--            </p>-->
    <!--            <video id="v0" width="100%" autoplay loop muted>-->
    <!--                <source src="img/pe_anim_horiz.mp4" type="video/mp4"/>-->
    <!--            </video>-->
    <!--            <p class="text-justify">-->
    <!--                Here, we show how these feature vectors change as a function of a point moving in 1D space.-->
    <!--                <br><br>-->
    <!--                Our <em>integrated positional encoding</em> considers Gaussian <em>regions</em> of space, rather than-->
    <!--                infinitesimal points. This provides a natural way to input a "region" of space as query to a-->
    <!--                coordinate-based neural network, allowing the network to reason about sampling and aliasing. The-->
    <!--                expected value of each positional encoding component has a simple closed form:-->
    <!--            </p>-->
    <!--            <p style="text-align:center;">-->
    <!--                <img src="img/ipe_eqn_under_pad.png" height="30px" class="img-responsive">-->
    <!--            </p>-->
    <!--            <video id="v0" width="100%" autoplay loop muted>-->
    <!--                <source src="img/ipe_anim_horiz.mp4" type="video/mp4"/>-->
    <!--            </video>-->
    <!--            <p class="text-justify">-->
    <!--                We can see that when considering a wider region, the higher frequency features automatically shrink-->
    <!--                toward zero, providing the network with lower-frequency inputs. As the region narrows, these features-->
    <!--                converge to the original positional encoding.-->
    <!--            </p>-->
    <!--        </div>-->
    <!--    </div>-->


    <!--    <div class="row">-->
    <!--        <div class="col-md-8 col-md-offset-2">-->
    <!--            <h3>-->
    <!--                Mip-NeRF-->
    <!--            </h3>-->
    <!--            <p class="text-justify">-->
    <!--                We use integrated positional encoding to train NeRF to generate anti-aliased renderings. Rather than-->
    <!--                casting an infinitesimal ray through each pixel, we instead cast a full 3D <em>cone</em>. For each-->
    <!--                queried point along a ray, we consider its associated 3D conical frustum. Two different cameras viewing-->
    <!--                the same point in space may result in vastly different conical frustums, as illustrated here in 2D:-->
    <!--            </p>-->
    <!--            <p style="text-align:center;">-->
    <!--                <img src="img/scales_toy.png" class="img-responsive" alt="scales">-->
    <!--            </p>-->
    <!--            <p class="text-justify">-->
    <!--                In order to pass this information through the NeRF network, we fit a multivariate Gaussian to the-->
    <!--                conical frustum and use the integrated positional encoding described above to create the input feature-->
    <!--                vector to the network.-->
    <!--            </p>-->
    <!--        </div>-->
    <!--    </div>-->


    <!--    <div class="row">-->
    <!--        <div class="col-md-8 col-md-offset-2">-->
    <!--            <h3>-->
    <!--                Results-->
    <!--            </h3>-->
    <!--            <p class="text-justify">-->
    <!--                We train NeRF and mip-NeRF on a dataset with images at four different resolutions. Normal NeRF (left) is-->
    <!--                not capable of learning to represent the same scene at multiple levels of detail, with blurring in-->
    <!--                close-up shots and aliasing in low resolution views, while mip-NeRF (right) both preserves sharp details-->
    <!--                in close-ups and correctly renders the zoomed-out images.-->
    <!--            </p>-->
    <!--            <br>-->
    <!--            <video id="v0" width="100%" autoplay loop muted controls>-->
    <!--                <source src="img/ship_sbs_path1.mp4" type="video/mp4"/>-->
    <!--            </video>-->
    <!--            <video id="v0" width="100%" autoplay loop muted controls>-->
    <!--                <source src="img/chair_sbs_path1.mp4" type="video/mp4"/>-->
    <!--            </video>-->
    <!--            <video id="v0" width="100%" autoplay loop muted controls>-->
    <!--                <source src="img/lego_sbs_path1.mp4" type="video/mp4"/>-->
    <!--            </video>-->
    <!--            <video id="v0" width="100%" autoplay loop muted controls>-->
    <!--                <source src="img/mic_sbs_path1.mp4" type="video/mp4"/>-->
    <!--            </video>-->
    <!--            <br><br>-->
    <!--            <p class="text-justify">-->
    <!--                We can also manipulate the integrated positional encoding by using a larger or smaller radius than the-->
    <!--                true pixel footprint, exposing the continuous level of detail learned within a single network:-->
    <!--            </p>-->
    <!--            <video id="v0" width="100%" autoplay loop muted controls>-->
    <!--                <source src="img/lego_radii_manip_slider_200p.mp4" type="video/mp4"/>-->
    <!--            </video>-->
    <!--        </div>-->
    <!--    </div>-->


    <!--    <div class="row">-->
    <!--        <div class="col-md-8 col-md-offset-2">-->
    <!--            <h3>-->
    <!--                Related links-->
    <!--            </h3>-->
    <!--            <p class="text-justify">-->
    <!--                <a href="https://en.wikipedia.org/wiki/Spatial_anti-aliasing">Wikipedia</a> provides an excellent-->
    <!--                introduction to spatial anti-aliasing techniques.-->
    <!--            </p>-->
    <!--            <p class="text-justify">-->
    <!--                Mipmaps were introduced by Lance Williams in his paper "Pyramidal Parametrics" (<a-->
    <!--                    href="https://software.intel.com/sites/default/files/m/7/2/c/p1-williams.pdf">Williams (1983)</a>).-->
    <!--            </p>-->
    <!--            <p class="text-justify">-->
    <!--                <a href="https://dl.acm.org/doi/abs/10.1145/964965.808589">Amanatides (1984)</a> first proposed the idea-->
    <!--                of replacing rays with cones in computer graphics rendering.-->
    <!--            </p>-->
    <!--            <p class="text-justify">-->
    <!--                The closely related concept of <em>ray differentials</em> (<a-->
    <!--                    href="https://graphics.stanford.edu/papers/trd/">Igehy (1999)</a>) is used in most modern renderers-->
    <!--                to antialias textures and other material buffers during ray tracing.-->
    <!--            </p>-->
    <!--            <p class="text-justify">-->
    <!--                Cone tracing has been used along with prefiltered voxel-based representations of scene geometry for-->
    <!--                speeding up indirect illumination calculations in <a-->
    <!--                    href="https://research.nvidia.com/sites/default/files/publications/GIVoxels-pg2011-authors.pdf">Crassin-->
    <!--                et al. (2011)</a>.-->
    <!--            </p>-->
    <!--            <p class="text-justify">-->
    <!--                Mip-NeRF was implemented on top of the <a-->
    <!--                    href="https://github.com/google-research/google-research/tree/master/jaxnerf">JAXNeRF</a> codebase.-->
    <!--            </p>-->
    <!--        </div>-->
    <!--    </div>-->


    <div class="row">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                Citation
            </h3>
            <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@article{ghadimiatigh2022hyperbolic,
  title={Hyperbolic Image Segmentation},
  author={GhadimiAtigh, Mina and Schoep, Julian and Acar, Erman and van Noord, Nanne and Mettes, Pascal},
  journal={arXiv preprint arXiv:2203.05898},
  year={2022}
}</textarea>
            </div>
        </div>
    </div>

    <!--    <div class="row">-->
    <!--        <div class="col-md-8 col-md-offset-2">-->
    <!--            <h3>-->
    <!--                Acknowledgements-->
    <!--            </h3>-->
    <!--            <p class="text-justify">-->
    <!--                We thank Janne Kontkanen and David Salesin for their comments on the text, Paul Debevec for constructive-->
    <!--                discussions, and Boyang Deng for JaxNeRF.-->
    <!--                <br>-->
    <!--                MT is funded by an NSF Graduate Fellowship.-->
    <!--                <br>-->
    <!--                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a>.-->
    <!--            </p>-->
    <!--        </div>-->
    <!--    </div>-->
</div>
</body>
</html>
